{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retriever\n",
    "\n",
    "> Functionalities to Retrieve the publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import pandas as pd\n",
    "from datetime import datetime, date, timedelta\n",
    "from fastcore.all import *\n",
    "from pubmed_lib.data import *\n",
    "from pubmed_lib.parser import *\n",
    "# from pubmed_lib.core import *\n",
    "from pubmed_lib.viz import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "pd.set_option('display.max_columns',30)\n",
    "pd.set_option('display.max_colwidth',500)\n",
    "pd.set_option('display.max_rows',500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def getParsedArticles(query, #Query to Pubmed, by default searching Author, give a valid 'search_tag' to search by different tag\n",
    "                      f_year = datetime.now().year, #The max year to search papers from, default CY\n",
    "                      years = 3, #Window of years back to search papers\n",
    "                      **kwargs\n",
    "                     ):\n",
    "    cy = f_year\n",
    "    my = cy - years\n",
    "    results = searchpb(query, mindate=my, maxdate=cy, **kwargs)\n",
    "    id_list = results['IdList']\n",
    "    if len(id_list) == 0:\n",
    "        return 0\n",
    "    papers = fetch_details(id_list)\n",
    "    n_papers = len(id_list)\n",
    "    print('checking in {} Articles'.format(n_papers))\n",
    "    articles=[]\n",
    "    for i, paperinfo in enumerate(papers['PubmedArticle']):\n",
    "        article = parse_paperinfo(paperinfo)\n",
    "        if int(article['published']) < my:\n",
    "            continue\n",
    "        articles.append(article)\n",
    "    print(f'Keeping with {len(articles)} from last {years} years')\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def getParsedArticlesPeriod(query, #Query to Pubmed, by default searching Author, give a valid 'search_tag' to search by different tag\n",
    "                            maxdate=datetime.now().year, #The max year to search papers from, default CY\n",
    "                            years = 3, #Window of years back to search papers\n",
    "                            top_n=None, #Number of papers to retreive, ordered by published date\n",
    "                            **kwargs):\n",
    "    query = query + '[Author]'\n",
    "    results = searchpb(query, 1000, maxdate = maxdate, mindate = maxdate - years, **kwargs)\n",
    "    id_list = results['IdList']\n",
    "    if len(id_list) == 0:\n",
    "        return ([],0)\n",
    "    papers = fetch_details(id_list)\n",
    "    n_papers = len(id_list)\n",
    "    if verbose:\n",
    "        print('checking in {} Articles'.format(n_papers))\n",
    "    articles=[]\n",
    "    for i, paperinfo in enumerate(papers['PubmedArticle']):\n",
    "        article = parse_paperinfo(paperinfo)\n",
    "        if int(article['published']) < maxdate - years :\n",
    "            # print('to old, article published on {}'.format(article['published']))\n",
    "            continue\n",
    "        articles.append(article)\n",
    "    if len(articles) == 0:\n",
    "        if verbose:\n",
    "            print('No articles in the time period')\n",
    "        return ([],n_papers)\n",
    "    elif top_n:\n",
    "        df = pd.DataFrame(articles).sort_values('published', ascending=False)\n",
    "        df = df.iloc[:top_n]\n",
    "        articles = df.to_dict('records')\n",
    "    if verbose:\n",
    "        print('Keeping with {} from last {} years'.format(len(articles), years))\n",
    "    return (articles, n_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def fetchPubmedArticles(query, start, end, path, db_path = '/Volumes/Users/matu/Documents/Xcode/SFDC/db.pckl'):\n",
    "    \"\"\"Function to search in pubmed by query, start and end year.\n",
    "    It checks first in the database of abstracts downloaded before.\n",
    "    Create a csv file with the parsed pubmed results including abstract, authors, etc. (look at pubmed_utils)\n",
    "\n",
    "    return (pd.Dataframe) -> the DataFrame with all the information retrieved\"\"\"\n",
    "    db = loadDB(db_path)\n",
    "    if query not in db:\n",
    "        print('adding new year {} for {}'.format(start, query))\n",
    "        (pubmedData, total) = getParsedArticlesPeriod(query, start, end)\n",
    "        if pubmedData == 0:\n",
    "            db.update({query: {str(start):[total, 0]}})\n",
    "            return \n",
    "        else:\n",
    "            db.update({query: {str(start):[total, len(pubmedData)]}})\n",
    "    else:\n",
    "        if (str(start) in db[query]) and (str(end) in db[query]):\n",
    "            print(f\"{query} already in DB with year {start} - {end}, passing\")\n",
    "            df = pd.read_csv('{}/{}_{}_{}.csv'.format(path, query, start, start - end))\n",
    "            return df\n",
    "        else:\n",
    "            (pubmedData, total) = getParsedArticlesPeriod(query, start, end)\n",
    "            if pubmedData == 0:\n",
    "                db[query].update({str(start):[total, 0]})\n",
    "                return \n",
    "            else:\n",
    "                db[query].update({str(start):[total, len(pubmedData)]})\n",
    "    df = pd.DataFrame(pubmedData)\n",
    "    file_output = '{}/{}_{}_{}.csv'.format(path, query, start, start - end)\n",
    "    df.to_csv(file_output)\n",
    "    saveDB(db, db_path)\n",
    "    if df.shape[0] >= 10:\n",
    "        df = df.sort_values('published', ascending=False)\n",
    "        print('Using the 10 newer papers')\n",
    "        return df.iloc[:10]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def retrieveArticles():\n",
    "    results = searchpb('Peter Ihnat[Author]')\n",
    "    papers = fetch_details(results['IdList'])\n",
    "    articles=[]\n",
    "    for i, paperinfo in enumerate(papers['PubmedArticle']):\n",
    "        article = parse_paperinfo(paperinfo)\n",
    "        articles.append(article)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def get_email(query: str, **kwargs):\n",
    "    results = searchpb(query, **kwargs)\n",
    "    id_list = results['IdList']\n",
    "    summary = []\n",
    "    if len(id_list) == 0:\n",
    "        print('No existen articulos nuevos para esta consulta')\n",
    "        return\n",
    "    print('Existen ' + str(len(id_list)) + ' articulos')\n",
    "    search_details = fetch_details(id_list)\n",
    "    papers = search_details['PubmedArticle']\n",
    "    for paperinfo in papers:\n",
    "        article = parse_paperinfo(paperinfo)\n",
    "        summary.append(article)\n",
    "    pd_summary = pd.DataFrame(summary)\n",
    "###    pd_summary.to_excel(query.replace(' ','_') + '.xlsx')\n",
    "    pd_det = pd_summary.explode('autorlist')\n",
    "    pd_det = pd_det.reset_index(drop=True)\n",
    "    pd_det = pd.concat([pd_det['autorlist'].apply(pd.Series), pd_det.drop(['autorlist'], axis=1)], axis=1)\n",
    "    # pd_det.to_excel(query.replace(' ','_') + '_det.xlsx')\n",
    "    plot_countries(pd_det)\n",
    "    plot_timeline(pd_summary)\n",
    "#     df_filt =pipeline(pd_det, region)\n",
    "#     net, df = pd2ng(df_filt)\n",
    "#     net, df = measure_network(net, df)\n",
    "#     top50 = df.sort_values('paper',ascending=False).index[:50]\n",
    "#     # plot_circos(top50, net, table_name)\n",
    "#     plot_histogram(df, table_name)\n",
    "#     # pd_det2 = pd_summary.explode('autorlist')\n",
    "#     pd_det2 = pd_det[['Fname','Lname','emails','affiliations','countries','state',\n",
    "#                       'identifier','name','title','doi','pubmed','published']]\n",
    "#     df_cutoff = df.join(pd_det2.set_index('name'))\n",
    "#     writer_cutoff = pd.ExcelWriter(table_name + '_cut' + '.xlsx', engine='xlsxwriter')\n",
    "#     df_cutoff = df_cutoff.sort_values('paper', ascending=False)\n",
    "#     df_cutoff.to_excel(writer_cutoff)\n",
    "#     writer_cutoff.close()\n",
    "#     if aff:\n",
    "#         return pd_det2.loc[(pd_det2.emails != '')].sort_values('published',ascending=False)\n",
    "#     elif all_res:\n",
    "#         return pd_det2.loc[(pd_det2.emails != '')].drop_duplicates(['Fname', 'Lname','emails']).sort_values('published',ascending=False)\n",
    "                    \n",
    "#     return pd_det2.loc[(pd_det2.emails != '') & (pd_det2.Lname.str.contains(query.split(' ')[-1]))]#sort_values('published',ascending=False)\n",
    "    return pd_det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pubmed",
   "language": "python",
   "name": "pubmed"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
